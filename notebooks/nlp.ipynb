{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Architecture\n",
    "\n",
    "A brief introduction and demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain\n",
    "LangChain is a framework for developing applications powered by language models. \n",
    "\n",
    "It's very useful to build RAG pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This document has 2 pages\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"../data/unstructured/pdfs/Fly devs.pdf\")\n",
    "data = loader.load()\n",
    "print(f\"This document has {len(data)} pages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text splitters\n",
    "\n",
    "We need to break Documents into smaller chunks because is useful both, for indexing data and passing it to a model, large chunks are harder to search over and won't fit in model's context window.\n",
    "\n",
    "It's recommended chunks between 500 and 1000 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now we have 5 documents!\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 500,\n",
    "    length_function=len,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "documents = text_splitter.split_documents(data)\n",
    "print(f\"Now we have {len(documents)} documents!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Instantiate OpenAI Embeddings\n",
    "embedding_openai = OpenAIEmbeddings(model = \"text-embedding-ada-002\")\n",
    "\n",
    "# Create Vector Store with Embeddings\n",
    "vector_store = Chroma.from_documents(\n",
    "    documents = documents,\n",
    "    embedding = embedding_openai\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it with a query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = Chroma(\n",
    "    persist_directory = \"../data/data_embeddings/attention-embeddings\",\n",
    "    embedding_function = embedding_openai\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content='Minimum Viable Product (MVPs) to enable faster time to market, attract early adopters, \\nand achieve product -market fit from early on. The most trustworthy companies and \\npowerful start -ups had partnered with us to get real value, run operations efficiently, and \\ngain flexibility. Our goal is to enhance the growth of your business by establishing a \\nsuccessful partnership . \\n \\nWhat we do  at fly devs  \\nInnovative and unique software solutions, custom -tailored to the needs of your business.', metadata={'page': 0, 'source': '../data/unstructured/pdfs/Fly devs.pdf'}),\n",
       "  0.5469162464141846),\n",
       " (Document(page_content='Minimum Viable Product (MVPs) to enable faster time to market, attract early adopters, \\nand achieve product -market fit from early on. The most trustworthy companies and \\npowerful start -ups had partnered with us to get real value, run operations efficiently, and \\ngain flexibility. Our goal is to enhance the growth of your business by establishing a \\nsuccessful partnership . \\n \\nWhat we do  at fly devs  \\nInnovative and unique software solutions, custom -tailored to the needs of your business.', metadata={'page': 0, 'source': '../data/unstructured/pdfs/Fly devs.pdf'}),\n",
       "  0.5469162464141846),\n",
       " (Document(page_content='Product development : Connect with Flydevs for the opportunity to receive a customized \\nsoftware product that addresses your specific business needs.  \\nIT Staff Outsourcing : Build a global technology partnership with the help of our project \\noutsourcing service to allocate resources cost -effectively and focus on complete \\ntransparency.  \\nFlydevs has top -notch IT professionals in all areas for your digital projects. Working as a team, \\nwe achieve results beyond our capabilities.', metadata={'page': 0, 'source': '../data/unstructured/pdfs/Fly devs.pdf'}),\n",
       "  0.5525680780410767),\n",
       " (Document(page_content='Product development : Connect with Flydevs for the opportunity to receive a customized \\nsoftware product that addresses your specific business needs.  \\nIT Staff Outsourcing : Build a global technology partnership with the help of our project \\noutsourcing service to allocate resources cost -effectively and focus on complete \\ntransparency.  \\nFlydevs has top -notch IT professionals in all areas for your digital projects. Working as a team, \\nwe achieve results beyond our capabilities.', metadata={'page': 0, 'source': '../data/unstructured/pdfs/Fly devs.pdf'}),\n",
       "  0.5525680780410767),\n",
       " (Document(page_content='Fly devs: We create innovative software solutions  \\ncustom -tailored to your business.  \\n \\nFly devs is a all-in-one digital software development agency based in Argentina, offering \\nstaff augmentation and end -to-end software solutions to leading brands. Helping \\nbusinesses to scale, unlock new levels of growth and reduce overall operational costs by \\naccessing a large talent pool of skilled professionals on a project basis. Developing', metadata={'page': 0, 'source': '../data/unstructured/pdfs/Fly devs.pdf'}),\n",
       "  0.5689394474029541)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What are transformers?\"\n",
    "\n",
    "docs = vector_store.similarity_search_with_score(query, k=5)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content='reduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been', metadata={'page': 1, 'source': '../data/unstructured/pdfs/atenttion_is_all_you_need.pdf'}),\n",
       "  0.39198678731918335),\n",
       " (Document(page_content='reduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been', metadata={'page': 1, 'source': '../data/unstructured/pdfs/atenttion_is_all_you_need.pdf'}),\n",
       "  0.39207226037979126),\n",
       " (Document(page_content='Attention( Q, K, V ) = softmax(QKT\\n√dk)V (1)\\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof1√dk. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is', metadata={'page': 3, 'source': '../data/unstructured/pdfs/atenttion_is_all_you_need.pdf'}),\n",
       "  0.39310941100120544),\n",
       " (Document(page_content='Attention( Q, K, V ) = softmax(QKT\\n√dk)V (1)\\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof1√dk. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is', metadata={'page': 3, 'source': '../data/unstructured/pdfs/atenttion_is_all_you_need.pdf'}),\n",
       "  0.3931833803653717),\n",
       " (Document(page_content='but\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14', metadata={'page': 13, 'source': '../data/unstructured/pdfs/atenttion_is_all_you_need.pdf'}),\n",
       "  0.3932335078716278)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is attention?\"\n",
    "\n",
    "docs = vector_store.similarity_search_with_score(query, k=5)\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/josealcocer27/.cache/pypoetry/virtualenvs/pycon-colombia-2024-rDJfgf35-py3.11/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Minimum Viable Product (MVPs) to enable faster time to market, attract early adopters, \\nand achieve product -market fit from early on. The most trustworthy companies and \\npowerful start -ups had partnered with us to get real value, run operations efficiently, and \\ngain flexibility. Our goal is to enhance the growth of your business by establishing a \\nsuccessful partnership . \\n \\nWhat we do  at fly devs  \\nInnovative and unique software solutions, custom -tailored to the needs of your business.', metadata={'page': 0, 'source': '../data/unstructured/pdfs/Fly devs.pdf'}),\n",
       " Document(page_content='Product development : Connect with Flydevs for the opportunity to receive a customized \\nsoftware product that addresses your specific business needs.  \\nIT Staff Outsourcing : Build a global technology partnership with the help of our project \\noutsourcing service to allocate resources cost -effectively and focus on complete \\ntransparency.  \\nFlydevs has top -notch IT professionals in all areas for your digital projects. Working as a team, \\nwe achieve results beyond our capabilities.', metadata={'page': 0, 'source': '../data/unstructured/pdfs/Fly devs.pdf'}),\n",
       " Document(page_content='Fly devs: We create innovative software solutions  \\ncustom -tailored to your business.  \\n \\nFly devs is a all-in-one digital software development agency based in Argentina, offering \\nstaff augmentation and end -to-end software solutions to leading brands. Helping \\nbusinesses to scale, unlock new levels of growth and reduce overall operational costs by \\naccessing a large talent pool of skilled professionals on a project basis. Developing', metadata={'page': 0, 'source': '../data/unstructured/pdfs/Fly devs.pdf'}),\n",
       " Document(page_content='we achieve results beyond our capabilities.  \\nContact us : \\nhello@flydevs.com  \\n+54 911 6820 9610  \\n \\nARGENTINA  \\nAlfredo Bayardi 2569 5B, Castelar, Buenos Aires  \\n+54 911 6820 9610  \\nhello@flydevs.com  \\nAUSTRALIA  \\nSuite 4, 120 Erina Street, Gosford, NSW', metadata={'page': 0, 'source': '../data/unstructured/pdfs/Fly devs.pdf'})]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vector_store.as_retriever(\n",
    "    search_kwargs = {\"k\":4}\n",
    ")\n",
    "query = \"What are transformers?\"\n",
    "retriever.get_relevant_documents(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain.chains.qa_with_sources.retrieval import RetrievalQAWithSourcesChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model_name = \"gpt-3.5-turbo\",\n",
    "    temperature = 0.0,\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "qa_chain_with_sources = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm = llm,\n",
    "    chain_type = \"stuff\",\n",
    "    retriever = retriever,\n",
    "    return_source_documents=True\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can create a Chain to ask question based on our base knowledge (Vector Store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/josealcocer27/.cache/pypoetry/virtualenvs/pycon-colombia-2024-rDJfgf35-py3.11/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model_name = \"gpt-3.5-turbo\",\n",
    "    temperature = 0.0\n",
    ")\n",
    "\n",
    "qa_chain_with_sources = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm = llm,\n",
    "    chain_type = \"stuff\",\n",
    "    retriever = retriever,\n",
    "    return_source_documents=True,\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use our RAG pipeline 😎"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQAWithSourcesChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'What are transformers?',\n",
       " 'answer': 'Transformers are models that rely entirely on self-attention to compute representations of input and output without using sequence-aligned RNNs or convolution.\\n',\n",
       " 'sources': '../data/unstructured/pdfs/atenttion_is_all_you_need.pdf',\n",
       " 'source_documents': [Document(page_content='The Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic building', metadata={'page': 1, 'source': '../data/unstructured/pdfs/atenttion_is_all_you_need.pdf'}),\n",
       "  Document(page_content='The Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic building', metadata={'page': 1, 'source': '../data/unstructured/pdfs/atenttion_is_all_you_need.pdf'}),\n",
       "  Document(page_content='language modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture', metadata={'page': 1, 'source': '../data/unstructured/pdfs/atenttion_is_all_you_need.pdf'}),\n",
       "  Document(page_content='language modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture', metadata={'page': 1, 'source': '../data/unstructured/pdfs/atenttion_is_all_you_need.pdf'})]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain_with_sources.invoke({\"question\":\"What are transformers?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "from typing import Dict, Any\n",
    "import streamlit as st\n",
    "from chromadb.api.models.Collection import Collection\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.vectorstores import VectorStoreRetriever\n",
    "from langchain.chains.qa_with_sources.retrieval import RetrievalQAWithSourcesChain\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chains.conversational_retrieval.base import BaseConversationalRetrievalChain\n",
    "from langchain.prompts import SystemMessagePromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_docs_template_customer_service = \"\"\"You are a Customer Service Agent from the {company} company and solve questions about the context provided. You can answer common questions\n",
    "as how are you and things that people use in normal conversations, but when you receive questions about products, services or something else you have \n",
    "to answer only based in the context provided, so you can introduce your self as Customer Service Agent from the {company} company but only when someone asks who are you.\n",
    "Use the following pieces of context to answer the user's question.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "----------------\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:\"\n",
    "\"\"\"\n",
    "\n",
    "template_customer_service = '''\n",
    "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know.\n",
    "Don't try to make up an answer. The context could be in any language, but you will always answer in the language of the user's question.\n",
    "You have to act as Customer Service Agent from the HugggingFace company and solve questions about the context provided. You can answer common questions\n",
    "as how are you and thins that people use in normal conversations, but when you receive questions about products, services or something else you have \n",
    "to answer only based in the context provided.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\n",
    "'''\n",
    "\n",
    "\n",
    "template_customer_service2 = '''\n",
    "You have to act as Customer Service Agent from the {company} company and solve questions about the context provided. You can answer common questions\n",
    "as how are you and thins that people use in normal conversations, but when you receive questions about products, services or something else you have \n",
    "to answer only based in the context provided.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.streaming_aiter import AsyncIteratorCallbackHandler\n",
    "from langchain.callbacks.streaming_stdout_final_only import FinalStreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "llm = ChatOpenAI(\n",
    "    model_name = \"gpt-3.5-turbo\",\n",
    "    temperature = 0.0,\n",
    "    streaming=True\n",
    ")\n",
    "condense_question_llm = ChatOpenAI(\n",
    "    model_name = \"gpt-3.5-turbo\",\n",
    "    temperature = 0.0\n",
    ")\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    # memory=memory,\n",
    "    return_generated_question=False,\n",
    "    retriever=retriever,\n",
    "    condense_question_llm = condense_question_llm,\n",
    "    #condense_question_prompt=PromptTemplate.from_template(custom_template),\n",
    ")\n",
    "qa.combine_docs_chain.llm_chain.prompt.messages[0] = SystemMessagePromptTemplate.from_template(combine_docs_template_customer_service.replace(\"{company}\", \"Fly devs\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/josealcocer27/.cache/pypoetry/virtualenvs/pycon-colombia-2024-rDJfgf35-py3.11/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a Customer Service Agent from the Fly devs company and solve questions about the context provided. You can answer common questions\n",
      "as how are you and things that people use in normal conversations, but when you receive questions about products, services or something else you have \n",
      "to answer only based in the context provided, so you can introduce your self as Customer Service Agent from the Fly devs company but only when someone asks who are you.\n",
      "Use the following pieces of context to answer the user's question.\n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "\n",
      "Product development : Connect with Flydevs for the opportunity to receive a customized \n",
      "software product that addresses your specific business needs.  \n",
      "IT Staff Outsourcing : Build a global technology partnership with the help of our project \n",
      "outsourcing service to allocate resources cost -effectively and focus on complete \n",
      "transparency.  \n",
      "Flydevs has top -notch IT professionals in all areas for your digital projects. Working as a team, \n",
      "we achieve results beyond our capabilities.\n",
      "\n",
      "Minimum Viable Product (MVPs) to enable faster time to market, attract early adopters, \n",
      "and achieve product -market fit from early on. The most trustworthy companies and \n",
      "powerful start -ups had partnered with us to get real value, run operations efficiently, and \n",
      "gain flexibility. Our goal is to enhance the growth of your business by establishing a \n",
      "successful partnership . \n",
      " \n",
      "What we do  at fly devs  \n",
      "Innovative and unique software solutions, custom -tailored to the needs of your business.\n",
      "\n",
      "we achieve results beyond our capabilities.  \n",
      "Contact us : \n",
      "hello@flydevs.com  \n",
      "+54 911 6820 9610  \n",
      " \n",
      "ARGENTINA  \n",
      "Alfredo Bayardi 2569 5B, Castelar, Buenos Aires  \n",
      "+54 911 6820 9610  \n",
      "hello@flydevs.com  \n",
      "AUSTRALIA  \n",
      "Suite 4, 120 Erina Street, Gosford, NSW\n",
      "\n",
      "Fly devs: We create innovative software solutions  \n",
      "custom -tailored to your business.  \n",
      " \n",
      "Fly devs is a all-in-one digital software development agency based in Argentina, offering \n",
      "staff augmentation and end -to-end software solutions to leading brands. Helping \n",
      "businesses to scale, unlock new levels of growth and reduce overall operational costs by \n",
      "accessing a large talent pool of skilled professionals on a project basis. Developing\n",
      "\n",
      "Question: Introduce yourself!\n",
      "Helpful Answer:\"\n",
      "\n",
      "Human: Introduce yourself!\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello! I am a Customer Service Agent from the Fly devs company. How can I assist you today?'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.run(\"Introduce yourself!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# chat completion llm\n",
    "llm = ChatOpenAI(\n",
    "    model_name='gpt-3.5-turbo',\n",
    "    temperature=0.0\n",
    ")\n",
    "# conversational memory\n",
    "conversational_memory = ConversationBufferWindowMemory(\n",
    "    memory_key='chat_history',\n",
    "    k=5,\n",
    "    return_messages=True\n",
    ")\n",
    "# retrieval qa chain\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name='Knowledge Base',\n",
    "        func=qa.invoke,\n",
    "        description=(\n",
    "            'use this tool when answering general knowledge queries to get '\n",
    "            'more information about the topic'\n",
    "        )\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/josealcocer27/.cache/pypoetry/virtualenvs/pycon-colombia-2024-rDJfgf35-py3.11/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 0.3.0. Use Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc. instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import initialize_agent\n",
    "\n",
    "agent = initialize_agent(\n",
    "    agent='chat-conversational-react-description',\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    max_iterations=3,\n",
    "    early_stopping_method='generate',\n",
    "    memory = memory,\n",
    "    get_chat_history=lambda h : h\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_customer_service2 = \"\"\"\n",
    "Assistant is a large language model trained by {company}.\n",
    "\n",
    "Assistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
    "\n",
    "Assistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
    "\n",
    "Assistant have to act as Customer Service Agent from the {company} company and solve questions about the context provided. He cans answer common questions\n",
    "as how are you and things that people use in normal conversations.\n",
    "\n",
    "Overall, Assistant is a powerful system that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.agent.llm_chain.prompt.messages[0].prompt.template = template_customer_service2.replace(\"{company}\", \"Fly Devs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.streaming_aiter import AsyncIteratorCallbackHandler\n",
    "from langchain.callbacks.streaming_stdout_final_only import FinalStreamingStdOutCallbackHandler\n",
    "from langchain.schema import LLMResult\n",
    "\n",
    "class AsyncCallbackHandler(AsyncIteratorCallbackHandler):\n",
    "    content: str = \"\"\n",
    "    final_answer: bool = False\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    async def on_llm_new_token(self, token: str, **kwargs: Any) -> None:\n",
    "        self.content += token\n",
    "        # if we passed the final answer, we put tokens in queue\n",
    "        if self.final_answer:\n",
    "            if '\"action_input\": \"' in self.content:\n",
    "                if token not in ['\"', \"}\"]:\n",
    "                    self.queue.put_nowait(token)\n",
    "        elif \"Final Answer\" in self.content:\n",
    "            self.final_answer = True\n",
    "            self.content = \"\"\n",
    "    \n",
    "    async def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:\n",
    "        if self.final_answer:\n",
    "            self.content = \"\"\n",
    "            self.final_answer = False\n",
    "            self.done.set()\n",
    "        else:\n",
    "            self.content = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "async def run_call(query: str, stream_it: AsyncCallbackHandler):\n",
    "    # assign callback handler\n",
    "    agent.agent.llm_chain.llm.callbacks = [stream_it]\n",
    "    # now query\n",
    "    await agent.acall(inputs={\"input\": query})\n",
    "\n",
    "async def create_gen(query: str, stream_it: AsyncCallbackHandler):\n",
    "    task = asyncio.create_task(run_call(query, stream_it))\n",
    "    async for token in stream_it.aiter():\n",
    "        yield token\n",
    "    await task\n",
    "\n",
    "\n",
    "stream_it = AsyncCallbackHandler()\n",
    "gen = create_gen(\"Who are you\", stream_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'async_generator' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgen\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m------\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'async_generator' object is not iterable"
     ]
    }
   ],
   "source": [
    "chunks = []\n",
    "for chunk in gen:\n",
    "    chunks.append(chunk)\n",
    "    print(\"------\")\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<async_generator object AgentExecutor.astream at 0x7f371709c280>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.astream({\"input\": \"Where is Fly Devs located?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m```json\n",
      "{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"I am Assistant, a large language model trained by Fly Devs to assist with a wide range of tasks and provide information on various topics.\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'who are you?',\n",
       " 'chat_history': [HumanMessage(content='Wher is Fly Devs locatead?'),\n",
       "  AIMessage(content='Fly Devs has locations in Argentina, Spain, the United States, and Australia.'),\n",
       "  HumanMessage(content='who are you?'),\n",
       "  AIMessage(content='I am Assistant, a large language model trained by Fly Devs to assist with a wide range of tasks and provide information on various topics.')],\n",
       " 'output': 'I am Assistant, a large language model trained by Fly Devs to assist with a wide range of tasks and provide information on various topics.'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.invoke({\"input\": \"Where is Fly Devs located?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This document has 15 pages\n",
      "Now we have 93 documents!\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = PyPDFLoader(\"../data/unstructured/pdfs/atenttion_is_all_you_need.pdf\")\n",
    "data = loader.load()\n",
    "print(f\"This document has {len(data)} pages\")\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 500,\n",
    "    length_function=len,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "documents = text_splitter.split_documents(data)\n",
    "print(f\"Now we have {len(documents)} documents!\")\n",
    "\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Instantiate OpenAI Embeddings\n",
    "embedding_openai = OpenAIEmbeddings(model = \"text-embedding-ada-002\")\n",
    "\n",
    "# Create Vector Store with Embeddings\n",
    "vector_store = Chroma.from_documents(\n",
    "    documents = documents,\n",
    "    embedding = embedding_openai\n",
    ")\n",
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "retriever_tool=create_retriever_tool(retriever,\"documents_search\",\n",
    "                      \"Search for information about attention is all you need!\")\n",
    "\n",
    "retriever_tool.name\n",
    "\n",
    "tools=[retriever_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant')),\n",
       " MessagesPlaceholder(variable_name='chat_history', optional=True),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')),\n",
       " MessagesPlaceholder(variable_name='agent_scratchpad')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import hub\n",
    "# Get the prompt to use - you can modify this!\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "prompt.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant'))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.messages[0]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_openai_tools_agent\n",
    "agent=create_openai_tools_agent(llm,tools,prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentExecutor(verbose=True, agent=RunnableMultiActionAgent(runnable=RunnableAssign(mapper={\n",
       "  agent_scratchpad: RunnableLambda(lambda x: format_to_openai_tool_messages(x['intermediate_steps']))\n",
       "})\n",
       "| ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]], 'agent_scratchpad': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'openai-functions-agent', 'lc_hub_commit_hash': 'a1655024b06afbd95d17449f21316291e0726f13dcfaf990cc0d18087ad689a5'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant')), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')), MessagesPlaceholder(variable_name='agent_scratchpad')])\n",
       "| RunnableBinding(bound=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7fed928fc050>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7fed9252b7d0>, model_name='gpt-3.5-turbo-0125', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy=''), kwargs={'tools': [{'type': 'function', 'function': {'name': 'documents_search', 'description': 'Search for information about attention is all you need!', 'parameters': {'type': 'object', 'properties': {'query': {'description': 'query to look up in retriever', 'type': 'string'}}, 'required': ['query']}}}]})\n",
       "| OpenAIToolsAgentOutputParser(), input_keys_arg=[], return_keys_arg=[], stream_runnable=True), tools=[Tool(name='documents_search', description='Search for information about attention is all you need!', args_schema=<class 'langchain_core.tools.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x7fed93887240>, retriever=VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x7fed92051a50>), document_prompt=PromptTemplate(input_variables=['page_content'], template='{page_content}'), document_separator='\\n\\n'), coroutine=functools.partial(<function _aget_relevant_documents at 0x7fed93887420>, retriever=VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x7fed92051a50>), document_prompt=PromptTemplate(input_variables=['page_content'], template='{page_content}'), document_separator='\\n\\n'))])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import AgentExecutor\n",
    "agent_executor=AgentExecutor(agent=agent,tools=tools,verbose=True)\n",
    "agent_executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `documents_search` with `{'query': 'attention is all you need'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mbut\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\n",
      "Full attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\n",
      "and 6. Note that the attentions are very sharp for this word.\n",
      "14\n",
      "\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\n",
      "Full attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\n",
      "and 6. Note that the attentions are very sharp for this word.\n",
      "14\n",
      "\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\n",
      "sentence. We give two such examples above, from two different heads from the encoder self-attention\n",
      "at layer 5 of 6. The heads clearly learned to perform different tasks.\n",
      "15\n",
      "\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\n",
      "sentence. We give two such examples above, from two different heads from the encoder self-attention\n",
      "at layer 5 of 6. The heads clearly learned to perform different tasks.\n",
      "15\u001b[0m\u001b[32;1m\u001b[1;3m\"Attention is all you need\" seems to be related to a concept in a document discussing attention heads and their application in anaphora resolution. The document mentions that the application of attention should be just what is missing. It also refers to attention heads exhibiting behavior related to the structure of sentences. If you need more detailed information or clarification, please let me know.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is attention is all you need?',\n",
       " 'output': '\"Attention is all you need\" seems to be related to a concept in a document discussing attention heads and their application in anaphora resolution. The document mentions that the application of attention should be just what is missing. It also refers to attention heads exhibiting behavior related to the structure of sentences. If you need more detailed information or clarification, please let me know.'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "agent_executor.invoke({\"input\":\"What is attention is all you need?\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-QupOUSK4-py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
