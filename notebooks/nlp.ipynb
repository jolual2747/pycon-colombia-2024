{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Architecture\n",
    "\n",
    "A brief introduction and demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain\n",
    "LangChain is a framework for developing applications powered by language models. \n",
    "\n",
    "It's very useful to build RAG pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This document has 15 pages\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"../data/unstructured/pdfs/atenttion_is_all_you_need.pdf\")\n",
    "data = loader.load()\n",
    "print(f\"This document has {len(data)} pages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text splitters\n",
    "\n",
    "We need to break Documents into smaller chunks because is useful both, for indexing data and passing it to a model, large chunks are harder to search over and won't fit in model's context window.\n",
    "\n",
    "It's recommended chunks between 500 and 1000 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now we have 93 documents!\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 500,\n",
    "    length_function=len,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "documents = text_splitter.split_documents(data)\n",
    "print(f\"Now we have {len(documents)} documents!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Qdrant\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# Instantiate OpenAI Embeddings\n",
    "embedding_openai = OpenAIEmbeddings(model = \"text-embedding-ada-002\")\n",
    "\n",
    "vector_store = Qdrant.from_documents(\n",
    "    documents = documents,\n",
    "    embedding = embedding_openai,\n",
    "    location = \":memory:\",\n",
    "    collection_name = \"transformers\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Instantiate OpenAI Embeddings\n",
    "embedding_openai = OpenAIEmbeddings(model = \"text-embedding-ada-002\")\n",
    "\n",
    "# Create Vector Store with Embeddings\n",
    "vector_store = Chroma.from_documents(\n",
    "    documents = documents,\n",
    "    embedding = embedding_openai\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it with a query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = Chroma(\n",
    "    persist_directory = \"../data/data_embeddings/attention-embeddings\",\n",
    "    embedding_function = embedding_openai\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content='The Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic building', metadata={'source': '../data/unstructured/pdfs/atenttion_is_all_you_need.pdf', 'page': 1, '_id': 'b7b29e59a72d4551b2a00a58e1a9dab7', '_collection_name': 'transformers'}),\n",
       "  0.8125454953207472),\n",
       " (Document(page_content='language modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture', metadata={'source': '../data/unstructured/pdfs/atenttion_is_all_you_need.pdf', 'page': 1, '_id': 'ff6d8087caa045ab850d7ad5dd1118f9', '_collection_name': 'transformers'}),\n",
       "  0.8099060487460894),\n",
       " (Document(page_content='7 Conclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014', metadata={'source': '../data/unstructured/pdfs/atenttion_is_all_you_need.pdf', 'page': 9, '_id': 'c61a839ed81c4dfba1e7d20e3e1005ea', '_collection_name': 'transformers'}),\n",
       "  0.8055247617585792),\n",
       " (Document(page_content='University of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,', metadata={'source': '../data/unstructured/pdfs/atenttion_is_all_you_need.pdf', 'page': 0, '_id': '3d8293b4c998498f9da595847fe87b6a', '_collection_name': 'transformers'}),\n",
       "  0.8003011248759376),\n",
       " (Document(page_content='tion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [ 2,19]. In all but a few cases [ 27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.', metadata={'source': '../data/unstructured/pdfs/atenttion_is_all_you_need.pdf', 'page': 1, '_id': '15d6157bba034500b29e57e9e3a158bd', '_collection_name': 'transformers'}),\n",
       "  0.799018773667564)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What are transformers?\"\n",
    "\n",
    "docs = vector_store.similarity_search_with_score(query, k=5)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content='reduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been', metadata={'page': 1, 'source': '../data/unstructured/pdfs/atenttion_is_all_you_need.pdf'}),\n",
       "  0.39198678731918335),\n",
       " (Document(page_content='reduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been', metadata={'page': 1, 'source': '../data/unstructured/pdfs/atenttion_is_all_you_need.pdf'}),\n",
       "  0.39207226037979126),\n",
       " (Document(page_content='Attention( Q, K, V ) = softmax(QKT\\n√dk)V (1)\\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof1√dk. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is', metadata={'page': 3, 'source': '../data/unstructured/pdfs/atenttion_is_all_you_need.pdf'}),\n",
       "  0.39310941100120544),\n",
       " (Document(page_content='Attention( Q, K, V ) = softmax(QKT\\n√dk)V (1)\\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof1√dk. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is', metadata={'page': 3, 'source': '../data/unstructured/pdfs/atenttion_is_all_you_need.pdf'}),\n",
       "  0.3931833803653717),\n",
       " (Document(page_content='but\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14', metadata={'page': 13, 'source': '../data/unstructured/pdfs/atenttion_is_all_you_need.pdf'}),\n",
       "  0.3932335078716278)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is attention?\"\n",
    "\n",
    "docs = vector_store.similarity_search_with_score(query, k=5)\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(\n",
    "    search_kwargs = {\"k\":4}\n",
    ")\n",
    "query = \"What are transformers?\"\n",
    "retriever.get_relevant_documents(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain.chains.qa_with_sources.retrieval import RetrievalQAWithSourcesChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can create a Chain to ask question based on our base knowledge (Vector Store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model_name = \"gpt-3.5-turbo\",\n",
    "    temperature = 0.0,\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "qa_chain_with_sources = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm = llm,\n",
    "    chain_type = \"stuff\",\n",
    "    retriever = retriever,\n",
    "    return_source_documents=True\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use our RAG pipeline 😎"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What are transformers?',\n",
       " 'answer': 'Transformers are a type of sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention. They allow for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight GPUs.\\n',\n",
       " 'sources': '../data/unstructured/pdfs/atenttion_is_all_you_need.pdf',\n",
       " 'source_documents': [Document(page_content='The Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic building', metadata={'page': 1, 'source': '../data/unstructured/pdfs/atenttion_is_all_you_need.pdf'}),\n",
       "  Document(page_content='language modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture', metadata={'page': 1, 'source': '../data/unstructured/pdfs/atenttion_is_all_you_need.pdf'}),\n",
       "  Document(page_content='7 Conclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014', metadata={'page': 9, 'source': '../data/unstructured/pdfs/atenttion_is_all_you_need.pdf'}),\n",
       "  Document(page_content='University of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,', metadata={'page': 0, 'source': '../data/unstructured/pdfs/atenttion_is_all_you_need.pdf'})]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain_with_sources.invoke({\"question\":\"What are transformers?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's use Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "from typing import Dict, Any\n",
    "import streamlit as st\n",
    "from chromadb.api.models.Collection import Collection\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.vectorstores import VectorStoreRetriever\n",
    "from langchain.chains.qa_with_sources.retrieval import RetrievalQAWithSourcesChain\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chains.conversational_retrieval.base import BaseConversationalRetrievalChain\n",
    "from langchain.prompts import SystemMessagePromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_docs_template_customer_service = \"\"\"You are a Customer Service Agent from the {company} company and solve questions about the context provided. You can answer common questions\n",
    "as how are you and things that people use in normal conversations, but when you receive questions about products, services or something else you have \n",
    "to answer only based in the context provided, so you can introduce your self as Customer Service Agent from the {company} company but only when someone asks who are you.\n",
    "Use the following pieces of context to answer the user's question.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "----------------\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:\"\n",
    "\"\"\"\n",
    "\n",
    "template_customer_service = '''\n",
    "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know.\n",
    "Don't try to make up an answer. The context could be in any language, but you will always answer in the language of the user's question.\n",
    "You have to act as Customer Service Agent from the HugggingFace company and solve questions about the context provided. You can answer common questions\n",
    "as how are you and thins that people use in normal conversations, but when you receive questions about products, services or something else you have \n",
    "to answer only based in the context provided.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\n",
    "'''\n",
    "\n",
    "\n",
    "template_customer_service2 = '''\n",
    "You have to act as Customer Service Agent from the {company} company and solve questions about the context provided. You can answer common questions\n",
    "as how are you and thins that people use in normal conversations, but when you receive questions about products, services or something else you have \n",
    "to answer only based in the context provided.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.streaming_aiter import AsyncIteratorCallbackHandler\n",
    "from langchain.callbacks.streaming_stdout_final_only import FinalStreamingStdOutCallbackHandler\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "# chat completion llm\n",
    "llm = ChatOpenAI(\n",
    "    model_name='gpt-3.5-turbo',\n",
    "    temperature=0.0,\n",
    "    streaming=True,\n",
    "    callbacks= [StreamingStdOutCallbackHandler()]\n",
    ")\n",
    "# conversational memory\n",
    "conversational_memory = ConversationBufferWindowMemory(\n",
    "    memory_key='chat_history',\n",
    "    k=5,\n",
    "    return_messages=True\n",
    ")\n",
    "# retrieval qa chain\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name='Knowledge Base',\n",
    "        func=qa.invoke,\n",
    "        description=(\n",
    "            'use this tool when answering general knowledge queries to get '\n",
    "            'more information about the topic'\n",
    "        )\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"knowledge_base\",\n",
    "    'use this tool when answering general knowledge queries to get more information about the topic'\n",
    ")\n",
    "tools = [tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/josealcocer27/.cache/pypoetry/virtualenvs/pycon-colombia-2024-rDJfgf35-py3.11/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `BaseTool.__call__` was deprecated in langchain-core 0.1.47 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic building\\n\\n7 Conclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\n\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture\\n\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool(\"Transformers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# conversational memory\n",
    "conversational_memory = ConversationBufferWindowMemory(\n",
    "    memory_key='chat_history',\n",
    "    k=5,\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "agent = initialize_agent(\n",
    "    agent='chat-conversational-react-description',\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    max_iterations=3,\n",
    "    early_stopping_method='generate',\n",
    "    memory = memory,\n",
    "    get_chat_history=lambda h : h,\n",
    "    return_intermediate_steps = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_customer_service2 = \"\"\"\n",
    "Assistant is a large language model trained by {company}.\n",
    "\n",
    "Assistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
    "\n",
    "Assistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
    "\n",
    "Assistant have to act as Customer Service Agent from the {company} company and solve questions about the context provided. He cans answer common questions\n",
    "as how are you and things that people use in normal conversations.\n",
    "\n",
    "Overall, Assistant is a powerful system that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.agent.llm_chain.prompt.messages[0].prompt.template = template_customer_service2.replace(\"{company}\", \"HuggingFace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "```json\n",
      "{\n",
      "    \"action\": \"Final \"Soy Assistant, un modelo de lenguaje creado por HuggingFace para ayudarte con una amplia variedad de tareas y preguntas. ¿En qué puedo ayudarte hoy?\"\n",
      "}\n",
      "```\u001b[32;1m\u001b[1;3m```json\n",
      "{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"Soy Assistant, un modelo de lenguaje creado por HuggingFace para ayudarte con una amplia variedad de tareas y preguntas. ¿En qué puedo ayudarte hoy?\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Quién eres?',\n",
       " 'chat_history': [HumanMessage(content='Quién eres?'),\n",
       "  AIMessage(content='Soy Assistant, un modelo de lenguaje creado por HuggingFace para ayudarte con una amplia variedad de tareas y preguntas. ¿En qué puedo ayudarte hoy?')],\n",
       " 'output': 'Soy Assistant, un modelo de lenguaje creado por HuggingFace para ayudarte con una amplia variedad de tareas y preguntas. ¿En qué puedo ayudarte hoy?'}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent(\"Quién eres?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.streaming_stdout_final_only import FinalStreamingStdOutCallbackHandler\n",
    "\n",
    "agent.agent.llm_chain.llm.callbacks = [\n",
    "    FinalStreamingStdOutCallbackHandler(\n",
    "        answer_prefix_tokens=[\"Final\", \"Answer\"]\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m```json\n",
      "{\n",
      "    \"action\": \"knowledge_base\",\n",
      "    \"action_input\": \"Transformers\"\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mThe Transformer allows for significantly more parallelization and can reach a new state of the art in\n",
      "translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
      "2 Background\n",
      "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n",
      "[16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic building\n",
      "\n",
      "7 Conclusion\n",
      "In this work, we presented the Transformer, the first sequence transduction model based entirely on\n",
      "attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\n",
      "multi-headed self-attention.\n",
      "For translation tasks, the Transformer can be trained significantly faster than architectures based\n",
      "on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\n",
      "\n",
      "language modeling tasks [34].\n",
      "To the best of our knowledge, however, the Transformer is the first transduction model relying\n",
      "entirely on self-attention to compute representations of its input and output without using sequence-\n",
      "aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\n",
      "self-attention and discuss its advantages over models such as [17, 18] and [9].\n",
      "3 Model Architecture\n",
      "\n",
      "University of Toronto\n",
      "aidan@cs.toronto.eduŁukasz Kaiser∗\n",
      "Google Brain\n",
      "lukaszkaiser@google.com\n",
      "Illia Polosukhin∗ ‡\n",
      "illia.polosukhin@gmail.com\n",
      "Abstract\n",
      "The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural networks that include an encoder and a decoder. The best\n",
      "performing models also connect the encoder and decoder through an attention\n",
      "mechanism. We propose a new simple network architecture, the Transformer,\u001b[0m\n",
      "Thought:\",\n",
      "    \"action_input\": \"The Transformer is a sequence transduction model based entirely on attention, replacing recurrent layers with multi-headed self-attention. It can be trained faster than architectures based on recurrent or convolutional layers and has shown state-of-the-art performance in translation tasks.\"\n",
      "}\n",
      "```\u001b[32;1m\u001b[1;3m```json\n",
      "{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"The Transformer is a sequence transduction model based entirely on attention, replacing recurrent layers with multi-headed self-attention. It can be trained faster than architectures based on recurrent or convolutional layers and has shown state-of-the-art performance in translation tasks.\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What are transformers?',\n",
       " 'chat_history': [HumanMessage(content='What are transformers?'),\n",
       "  AIMessage(content='The Transformer is a sequence transduction model based entirely on attention, replacing recurrent layers with multi-headed self-attention. It can be trained faster than architectures based on recurrent or convolutional layers and has shown state-of-the-art performance in translation tasks.'),\n",
       "  HumanMessage(content='What are transformers?'),\n",
       "  AIMessage(content='The Transformer is a sequence transduction model based entirely on attention, replacing recurrent layers with multi-headed self-attention. It can be trained faster than architectures based on recurrent or convolutional layers and has shown state-of-the-art performance in translation tasks.')],\n",
       " 'output': 'The Transformer is a sequence transduction model based entirely on attention, replacing recurrent layers with multi-headed self-attention. It can be trained faster than architectures based on recurrent or convolutional layers and has shown state-of-the-art performance in translation tasks.'}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent(\"What are transformers?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "class CallbackHandler(StreamingStdOutCallbackHandler):\n",
    "    def __init__(self):\n",
    "        self.content: str = \"\"\n",
    "        self.final_answer: bool = False\n",
    "\n",
    "    def on_llm_new_token(self, token: str, **kwargs: any) -> None:\n",
    "        self.content += token\n",
    "        if \"Final Answer\" in self.content:\n",
    "            # now we're in the final answer section, but don't print yet\n",
    "            self.final_answer = True\n",
    "            self.content = \"\"\n",
    "        if self.final_answer:\n",
    "            if '\"action_input\": \"' in self.content:\n",
    "                if token not in [\"}\"]:\n",
    "                    sys.stdout.write(token)  # equal to `print(token, end=\"\")`\n",
    "                    sys.stdout.flush()\n",
    "\n",
    "agent.agent.llm_chain.llm.callbacks = [CallbackHandler()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m```json\n",
      "{\n",
      "    \"action\": \"knowledge_base\",\n",
      "    \"action_input\": \"Transformers (machine learning model)\"\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mThe Transformer allows for significantly more parallelization and can reach a new state of the art in\n",
      "translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
      "2 Background\n",
      "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n",
      "[16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic building\n",
      "\n",
      "tion models in various tasks, allowing modeling of dependencies without regard to their distance in\n",
      "the input or output sequences [ 2,19]. In all but a few cases [ 27], however, such attention mechanisms\n",
      "are used in conjunction with a recurrent network.\n",
      "In this work we propose the Transformer, a model architecture eschewing recurrence and instead\n",
      "relying entirely on an attention mechanism to draw global dependencies between input and output.\n",
      "\n",
      "University of Toronto\n",
      "aidan@cs.toronto.eduŁukasz Kaiser∗\n",
      "Google Brain\n",
      "lukaszkaiser@google.com\n",
      "Illia Polosukhin∗ ‡\n",
      "illia.polosukhin@gmail.com\n",
      "Abstract\n",
      "The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural networks that include an encoder and a decoder. The best\n",
      "performing models also connect the encoder and decoder through an attention\n",
      "mechanism. We propose a new simple network architecture, the Transformer,\n",
      "\n",
      "language modeling tasks [34].\n",
      "To the best of our knowledge, however, the Transformer is the first transduction model relying\n",
      "entirely on self-attention to compute representations of its input and output without using sequence-\n",
      "aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\n",
      "self-attention and discuss its advantages over models such as [17, 18] and [9].\n",
      "3 Model Architecture\u001b[0m\n",
      "Thought: \"The Transformer model is a sequence transduction model based on attention, replacing recurrent layers with multi-headed self-attention. It can be trained faster than architectures based on recurrent or convolutional layers and has shown state-of-the-art performance in translation tasks.\"\n",
      "}\n",
      "```\u001b[32;1m\u001b[1;3m```json\n",
      "{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"The Transformer model is a sequence transduction model based on attention, replacing recurrent layers with multi-headed self-attention. It can be trained faster than architectures based on recurrent or convolutional layers and has shown state-of-the-art performance in translation tasks.\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What are transformers?',\n",
       " 'chat_history': [HumanMessage(content='What are transformers?'),\n",
       "  AIMessage(content='The Transformer is a sequence transduction model based entirely on attention, replacing recurrent layers with multi-headed self-attention. It can be trained faster than architectures based on recurrent or convolutional layers and has shown state-of-the-art performance in translation tasks.'),\n",
       "  HumanMessage(content='What are transformers?'),\n",
       "  AIMessage(content='The Transformer is a sequence transduction model based entirely on attention, replacing recurrent layers with multi-headed self-attention. It can be trained faster than architectures based on recurrent or convolutional layers and has shown state-of-the-art performance in translation tasks.'),\n",
       "  HumanMessage(content='What are transformers?'),\n",
       "  AIMessage(content='The Transformer model is a sequence transduction model based on attention, replacing recurrent layers with multi-headed self-attention. It can be trained faster than architectures based on recurrent or convolutional layers and has shown state-of-the-art performance in translation tasks.')],\n",
       " 'output': 'The Transformer model is a sequence transduction model based on attention, replacing recurrent layers with multi-headed self-attention. It can be trained faster than architectures based on recurrent or convolutional layers and has shown state-of-the-art performance in translation tasks.'}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent(\"What are transformers?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.streaming_aiter import AsyncIteratorCallbackHandler\n",
    "from langchain.callbacks.streaming_stdout_final_only import FinalStreamingStdOutCallbackHandler\n",
    "from langchain.schema import LLMResult\n",
    "\n",
    "class AsyncCallbackHandler(AsyncIteratorCallbackHandler):\n",
    "    content: str = \"\"\n",
    "    final_answer: bool = False\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    async def on_llm_new_token(self, token: str, **kwargs: Any) -> None:\n",
    "        self.content += token\n",
    "        # if we passed the final answer, we put tokens in queue\n",
    "        if self.final_answer:\n",
    "            if '\"action_input\": \"' in self.content:\n",
    "                if token not in ['\"', \"}\"]:\n",
    "                    self.queue.put_nowait(token)\n",
    "        elif \"Final Answer\" in self.content:\n",
    "            self.final_answer = True\n",
    "            self.content = \"\"\n",
    "    \n",
    "    async def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:\n",
    "        if self.final_answer:\n",
    "            self.content = \"\"\n",
    "            self.final_answer = False\n",
    "            self.done.set()\n",
    "        else:\n",
    "            self.content = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "async def run_call(query: str, stream_it: AsyncCallbackHandler):\n",
    "    # assign callback handler\n",
    "    agent.agent.llm_chain.llm.callbacks = [stream_it]\n",
    "    # now query\n",
    "    await agent.acall(inputs={\"input\": query})\n",
    "\n",
    "async def create_gen(query: str, stream_it: AsyncCallbackHandler):\n",
    "    task = asyncio.create_task(run_call(query, stream_it))\n",
    "    async for token in stream_it.aiter():\n",
    "        yield token\n",
    "    await task\n",
    "\n",
    "\n",
    "stream_it = AsyncCallbackHandler()\n",
    "gen = create_gen(\"Who are you\", stream_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'async_generator' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgen\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m------\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'async_generator' object is not iterable"
     ]
    }
   ],
   "source": [
    "chunks = []\n",
    "for chunk in gen:\n",
    "    chunks.append(chunk)\n",
    "    print(\"------\")\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<async_generator object AgentExecutor.astream at 0x7f371709c280>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.astream({\"input\": \"Where is Fly Devs located?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m```json\n",
      "{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"I am Assistant, a large language model trained by Fly Devs to assist with a wide range of tasks and provide information on various topics.\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'who are you?',\n",
       " 'chat_history': [HumanMessage(content='Wher is Fly Devs locatead?'),\n",
       "  AIMessage(content='Fly Devs has locations in Argentina, Spain, the United States, and Australia.'),\n",
       "  HumanMessage(content='who are you?'),\n",
       "  AIMessage(content='I am Assistant, a large language model trained by Fly Devs to assist with a wide range of tasks and provide information on various topics.')],\n",
       " 'output': 'I am Assistant, a large language model trained by Fly Devs to assist with a wide range of tasks and provide information on various topics.'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.invoke({\"input\": \"Where is Fly Devs located?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This document has 15 pages\n",
      "Now we have 93 documents!\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = PyPDFLoader(\"../data/unstructured/pdfs/atenttion_is_all_you_need.pdf\")\n",
    "data = loader.load()\n",
    "print(f\"This document has {len(data)} pages\")\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 500,\n",
    "    length_function=len,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "documents = text_splitter.split_documents(data)\n",
    "print(f\"Now we have {len(documents)} documents!\")\n",
    "\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Instantiate OpenAI Embeddings\n",
    "embedding_openai = OpenAIEmbeddings(model = \"text-embedding-ada-002\")\n",
    "\n",
    "# Create Vector Store with Embeddings\n",
    "vector_store = Chroma.from_documents(\n",
    "    documents = documents,\n",
    "    embedding = embedding_openai\n",
    ")\n",
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "retriever_tool=create_retriever_tool(retriever,\"documents_search\",\n",
    "                      \"Search for information about attention is all you need!\")\n",
    "\n",
    "retriever_tool.name\n",
    "\n",
    "tools=[retriever_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant')),\n",
       " MessagesPlaceholder(variable_name='chat_history', optional=True),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')),\n",
       " MessagesPlaceholder(variable_name='agent_scratchpad')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import hub\n",
    "# Get the prompt to use - you can modify this!\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "prompt.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant'))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.messages[0]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_openai_tools_agent\n",
    "agent=create_openai_tools_agent(llm,tools,prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentExecutor(verbose=True, agent=RunnableMultiActionAgent(runnable=RunnableAssign(mapper={\n",
       "  agent_scratchpad: RunnableLambda(lambda x: format_to_openai_tool_messages(x['intermediate_steps']))\n",
       "})\n",
       "| ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]], 'agent_scratchpad': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'openai-functions-agent', 'lc_hub_commit_hash': 'a1655024b06afbd95d17449f21316291e0726f13dcfaf990cc0d18087ad689a5'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant')), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')), MessagesPlaceholder(variable_name='agent_scratchpad')])\n",
       "| RunnableBinding(bound=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7fed928fc050>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7fed9252b7d0>, model_name='gpt-3.5-turbo-0125', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy=''), kwargs={'tools': [{'type': 'function', 'function': {'name': 'documents_search', 'description': 'Search for information about attention is all you need!', 'parameters': {'type': 'object', 'properties': {'query': {'description': 'query to look up in retriever', 'type': 'string'}}, 'required': ['query']}}}]})\n",
       "| OpenAIToolsAgentOutputParser(), input_keys_arg=[], return_keys_arg=[], stream_runnable=True), tools=[Tool(name='documents_search', description='Search for information about attention is all you need!', args_schema=<class 'langchain_core.tools.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x7fed93887240>, retriever=VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x7fed92051a50>), document_prompt=PromptTemplate(input_variables=['page_content'], template='{page_content}'), document_separator='\\n\\n'), coroutine=functools.partial(<function _aget_relevant_documents at 0x7fed93887420>, retriever=VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x7fed92051a50>), document_prompt=PromptTemplate(input_variables=['page_content'], template='{page_content}'), document_separator='\\n\\n'))])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import AgentExecutor\n",
    "agent_executor=AgentExecutor(agent=agent,tools=tools,verbose=True)\n",
    "agent_executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `documents_search` with `{'query': 'attention is all you need'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mbut\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\n",
      "Full attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\n",
      "and 6. Note that the attentions are very sharp for this word.\n",
      "14\n",
      "\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\n",
      "Full attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\n",
      "and 6. Note that the attentions are very sharp for this word.\n",
      "14\n",
      "\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\n",
      "sentence. We give two such examples above, from two different heads from the encoder self-attention\n",
      "at layer 5 of 6. The heads clearly learned to perform different tasks.\n",
      "15\n",
      "\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\n",
      "sentence. We give two such examples above, from two different heads from the encoder self-attention\n",
      "at layer 5 of 6. The heads clearly learned to perform different tasks.\n",
      "15\u001b[0m\u001b[32;1m\u001b[1;3m\"Attention is all you need\" seems to be related to a concept in a document discussing attention heads and their application in anaphora resolution. The document mentions that the application of attention should be just what is missing. It also refers to attention heads exhibiting behavior related to the structure of sentences. If you need more detailed information or clarification, please let me know.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is attention is all you need?',\n",
       " 'output': '\"Attention is all you need\" seems to be related to a concept in a document discussing attention heads and their application in anaphora resolution. The document mentions that the application of attention should be just what is missing. It also refers to attention heads exhibiting behavior related to the structure of sentences. If you need more detailed information or clarification, please let me know.'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "agent_executor.invoke({\"input\":\"What is attention is all you need?\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-QupOUSK4-py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
